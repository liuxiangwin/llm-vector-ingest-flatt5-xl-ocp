{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "165a3407",
   "metadata": {},
   "source": [
    "## Querying a Milvus index - Nomic AI Embeddings\n",
    "\n",
    "Simple example on how to query content from a Milvus VectorStore. In this example, the embeddings are the fully open source ones released by NomicAI, [nomic-embed-text-v1](https://huggingface.co/nomic-ai/nomic-embed-text-v1).\n",
    "\n",
    "As described in [this blog post](https://blog.nomic.ai/posts/nomic-embed-text-v1), those embeddings feature a \"8192 context-length that outperforms OpenAI Ada-002 and text-embedding-3-small on both short and long context tasks\". In additions, they are:\n",
    "\n",
    "- Open source\n",
    "- Open data\n",
    "- Open training code\n",
    "- Fully reproducible and auditable\n",
    "\n",
    "Requirements:\n",
    "- A Milvus instance, either standalone or cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ac3132-6929-4477-9585-31761d7d9848",
   "metadata": {},
   "source": [
    "### Needed packages and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ed97389-9c5b-46a8-bedf-f28bf7038a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q einops==0.7.0 langchain==0.1.9 pymilvus==2.3.6 sentence-transformers==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c53e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Milvus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b8ecae",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Base parameters, the Milvus connection info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9376e567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MILVUS_HOST = \"vectordb-milvus.milvus.svc.cluster.local\"\n",
    "MILVUS_PORT = 19530\n",
    "MILVUS_USERNAME = \"root\"\n",
    "MILVUS_PASSWORD = \"Milvus\"\n",
    "MILVUS_COLLECTION = \"collection_nomicai_embeddings\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d51868",
   "metadata": {},
   "source": [
    "### Initialize the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbb6a3e3-5ccd-441e-b80d-427555d9e9f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 2.4.0.dev0, however, your version is 2.4.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "!!!!!!!!!!!!megablocks not available, using torch.matmul instead\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# If you don't want to use a GPU, you can remove the 'device': 'cuda' argument\n",
    "model_kwargs = {'trust_remote_code': True, 'device': 'cuda'}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"nomic-ai/nomic-embed-text-v1\",\n",
    "    model_kwargs=model_kwargs,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"host\": MILVUS_HOST, \"port\": MILVUS_PORT, \"user\": MILVUS_USERNAME, \"password\": MILVUS_PASSWORD},\n",
    "    collection_name=MILVUS_COLLECTION,\n",
    "    metadata_field=\"metadata\",\n",
    "    text_field=\"page_content\",\n",
    "    drop_old=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9856851c",
   "metadata": {},
   "source": [
    "### Make a query to the index to verify sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9621e231-3541-40bc-85ef-8aa3b2ba2331",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26740b0ccb804ffa897664e52e5c36d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.13/html-single/working_on_data_science_projects/index\n",
      "https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.13/html-single/openshift_ai_tutorial_-_fraud_detection_example/index\n",
      "https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.13/html-single/getting_started_with_red_hat_openshift_ai_self-managed/index\n",
      "https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.13/html-single/working_on_data_science_projects/index\n"
     ]
    }
   ],
   "source": [
    "query=\"How can I create a Data Science Project?\"\n",
    "results = store.similarity_search_with_score(query, k=4, return_metadata=True)\n",
    "for result in results:\n",
    "    print(result[0].metadata['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1005e2c",
   "metadata": {},
   "source": [
    "### Work with a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "566f9347-a40a-4eeb-a690-e199b91947a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c378fbd-395d-43af-8cca-268bc05d0f51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e62b6f6d744d6dae8bd70d9996d691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='CHAPTER 1. USING DATA SCIENCE PROJECTS\\n1.1. CREATING A DATA SCIENCE PROJECT\\nTo implement a data science workflow, you must create a project. In OpenShift, a project is a Kubernetes\\nnamespace with additional annotations, and is the main way that you can manage user access to\\nresources. A project organizes your data science work in one place and also allows you to collaborate\\nwith other developers and data scientists in your organization.\\nWithin a project, you can add the following functionality:\\nData connections so that you can access data without having to hardcode information like\\nendpoints or credentials.\\nWorkbenches for working with and processing data, and for developing models.\\nDeployed models so that you can test them and then integrate them into intelligent\\napplications. Deploying a model makes it available as a service that you can access by using an\\nAPI.\\nPipelines for automating your ML workflow.\\nPrerequisites\\nYou have logged in to Red Hat OpenShift AI.', metadata={'source': 'https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.13/html-single/working_on_data_science_projects/index', 'page': 7}),\n",
       " Document(page_content='However, it would be a one-off Jupyter notebook run in isolation. To implement a data science\\nworkflow, you must create a data science project (as described in the following procedure).\\nProjects allow you and your team to organize and collaborate on resources within separated\\nnamespaces. From a project you can create multiple workbenches, each with their own IDE\\nenvironment (for example, JupyterLab), and each with their own data connections and cluster\\nstorage. In addition, the workbenches can share models and data with pipelines and model\\nservers.\\n2\\n. \\nIf you are using the Red Hat Developer Sandbox, you are provided with a default data science\\nproject (for example, \\nmyname-dev\\n). Select it and skip over the next step to the \\nVerification\\nsection.\\nIf you are using your own OpenShift cluster, click \\nCreate data science project\\n.\\n3\\n. \\nEnter a display name and description. Based on the display name, a resource name is\\nautomatically generated, but you can change it if you prefer.\\nVerification', metadata={'source': 'https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.13/html-single/openshift_ai_tutorial_-_fraud_detection_example/index', 'page': 9}),\n",
       " Document(page_content='CHAPTER 3. CREATING A DATA SCIENCE PROJECT\\nTo implement a data science workflow, you must create a project. In OpenShift, a project is a Kubernetes\\nnamespace with additional annotations, and is the main way that you can manage user access to\\nresources. A project organizes your data science work in one place and also allows you to collaborate\\nwith other developers and data scientists in your organization.\\nWithin a project, you can add the following functionality:\\nData connections so that you can access data without having to hardcode information like\\nendpoints or credentials.\\nWorkbenches for working with and processing data, and for developing models.\\nDeployed models so that you can test them and then integrate them into intelligent\\napplications. Deploying a model makes it available as a service that you can access by using an\\nAPI.\\nPipelines for automating your ML workflow.\\nPrerequisites\\nYou have logged in to Red Hat OpenShift AI.', metadata={'source': 'https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.13/html-single/getting_started_with_red_hat_openshift_ai_self-managed/index', 'page': 9}),\n",
       " Document(page_content='PREFACE\\nAs a data scientist, you can organize your data science work into a single project. A data science project\\nin OpenShift AI can consist of the following components:\\nWorkbenches\\nCreating a workbench allows you to add a Jupyter notebook to your project.\\nCluster storage\\nFor data science projects that require data retention, you can add cluster storage to the project.\\nData connections\\nAdding a data connection to your project allows you to connect data inputs to your workbenches.\\nPipelines\\nStandardize and automate machine learning workflows to enable you to further enhance and deploy\\nyour data science models.\\nModels and model servers\\nDeploy a trained data science model to serve intelligent applications. Your model is deployed with an\\nendpoint that allows applications to send requests to the model.\\nPREFACE\\n3', metadata={'source': 'https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.13/html-single/working_on_data_science_projects/index', 'page': 6})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(query)\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
